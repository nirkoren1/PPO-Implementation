{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c73dc8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f5d6250",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 24576, steps/s: 4519, Avg Reward: 6.38\n",
      "Global Step: 49152, steps/s: 4510, Avg Reward: 10.53\n",
      "Global Step: 73728, steps/s: 4512, Avg Reward: 19.15\n",
      "Global Step: 98304, steps/s: 4505, Avg Reward: 51.20\n",
      "Global Step: 122880, steps/s: 4505, Avg Reward: 79.90\n",
      "Global Step: 147456, steps/s: 4489, Avg Reward: 101.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nir.k/uni/PPO-Implementation/code/ppo_train.py\", line 318, in <module>\n",
      "    final_reward, reward_plot = ppo.train()\n",
      "                                ^^^^^^^^^^^\n",
      "  File \"/Users/nir.k/uni/PPO-Implementation/code/ppo_train.py\", line 167, in train\n",
      "    obs, reward, terminated, truncated, info = self.envs.step(action.cpu().numpy())\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nir.k/uni/PPO-Implementation/code/env_utils.py\", line 48, in step\n",
      "    return self.envs.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/gymnasium/wrappers/vector/common.py\", line 150, in step\n",
      "    ) = self.env.step(actions)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/gymnasium/vector/sync_vector_env.py\", line 265, in step\n",
      "    ) = self.envs[i].step(action)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 125, in step\n",
      "    observation, reward, terminated, truncated, info = self.env.step(action)\n",
      "                                                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 393, in step\n",
      "    return super().step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/gymnasium/core.py\", line 327, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/gymnasium/wrappers/common.py\", line 285, in step\n",
      "    return self.env.step(action)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/gymnasium/envs/mujoco/walker2d_v5.py\", line 287, in step\n",
      "    self.do_simulation(action, self.frame_skip)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/gymnasium/envs/mujoco/mujoco_env.py\", line 199, in do_simulation\n",
      "    self._step_mujoco_simulation(ctrl, n_frames)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/gymnasium/envs/mujoco/mujoco_env.py\", line 152, in _step_mujoco_simulation\n",
      "    mujoco.mj_rnePostConstraint(self.model, self.data)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_command = \"\"\"\n",
    "python ppo_train.py \\\n",
    "    --env-name \"Walker2d-v5\" \\\n",
    "    --state-encoder \"NoEncoder\" \\\n",
    "    --total-timesteps 10000000 \\\n",
    "    --num-steps 2048 \\\n",
    "    --update-epochs 10 \\\n",
    "    --minibatch-size 64 \\\n",
    "    --adam-stepsize 3e-4 \\\n",
    "    --gamma 0.99 \\\n",
    "    --gae-lambda 0.95 \\\n",
    "    --clip-coef 0.2 \\\n",
    "    --num-envs 12\n",
    "\"\"\"\n",
    "\n",
    "os.system(run_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_command = \"\"\"\n",
    "python ppo_train.py \\\n",
    "    --env-name \"Walker2d-v5\" \\\n",
    "    --state-encoder \"NoEncoder\" \\\n",
    "    --total-timesteps 50000000 \\\n",
    "    --num-envs 32 \\\n",
    "    --num-steps 512 \\\n",
    "    --update-epochs 15 \\\n",
    "    --minibatch-size 4096 \\\n",
    "    --loss-type \"kl_penalty\" \\\n",
    "    --adaptive-kl\n",
    "\"\"\"\n",
    "\n",
    "os.system(run_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66876afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 1024, steps/s: 213, Avg Reward: 320.00\n",
      "Global Step: 2048, steps/s: 213, Avg Reward: 367.78\n",
      "Global Step: 3072, steps/s: 213, Avg Reward: 350.00\n",
      "Global Step: 4096, steps/s: 213, Avg Reward: 314.58\n",
      "Global Step: 5120, steps/s: 209, Avg Reward: 331.88\n",
      "Global Step: 6144, steps/s: 209, Avg Reward: 346.84\n",
      "Global Step: 7168, steps/s: 208, Avg Reward: 339.78\n",
      "Global Step: 8192, steps/s: 207, Avg Reward: 339.00\n",
      "Global Step: 9216, steps/s: 207, Avg Reward: 333.97\n",
      "Global Step: 10240, steps/s: 207, Avg Reward: 325.15\n",
      "Global Step: 11264, steps/s: 206, Avg Reward: 320.00\n",
      "Global Step: 12288, steps/s: 206, Avg Reward: 319.62\n",
      "Global Step: 13312, steps/s: 205, Avg Reward: 321.19\n",
      "Global Step: 14336, steps/s: 205, Avg Reward: 316.60\n",
      "Global Step: 15360, steps/s: 204, Avg Reward: 313.60\n",
      "Global Step: 16384, steps/s: 205, Avg Reward: 308.60\n",
      "Global Step: 17408, steps/s: 205, Avg Reward: 314.50\n",
      "Global Step: 18432, steps/s: 206, Avg Reward: 341.20\n",
      "Global Step: 19456, steps/s: 205, Avg Reward: 340.00\n",
      "Global Step: 20480, steps/s: 205, Avg Reward: 337.60\n",
      "Global Step: 21504, steps/s: 204, Avg Reward: 335.70\n",
      "Global Step: 22528, steps/s: 204, Avg Reward: 333.70\n",
      "Global Step: 23552, steps/s: 204, Avg Reward: 334.20\n",
      "Global Step: 24576, steps/s: 204, Avg Reward: 349.30\n",
      "Global Step: 25600, steps/s: 204, Avg Reward: 351.60\n",
      "Global Step: 26624, steps/s: 205, Avg Reward: 351.70\n",
      "Global Step: 27648, steps/s: 205, Avg Reward: 357.80\n",
      "Global Step: 28672, steps/s: 205, Avg Reward: 362.50\n",
      "Global Step: 29696, steps/s: 205, Avg Reward: 376.10\n",
      "Global Step: 30720, steps/s: 205, Avg Reward: 380.40\n",
      "Global Step: 31744, steps/s: 205, Avg Reward: 379.60\n",
      "Global Step: 32768, steps/s: 205, Avg Reward: 382.40\n",
      "Global Step: 33792, steps/s: 206, Avg Reward: 375.80\n",
      "Global Step: 34816, steps/s: 205, Avg Reward: 379.30\n",
      "Global Step: 35840, steps/s: 205, Avg Reward: 379.60\n",
      "Global Step: 36864, steps/s: 205, Avg Reward: 379.30\n",
      "Global Step: 37888, steps/s: 205, Avg Reward: 382.50\n",
      "Global Step: 38912, steps/s: 205, Avg Reward: 389.50\n",
      "Global Step: 39936, steps/s: 205, Avg Reward: 391.70\n",
      "Global Step: 40960, steps/s: 204, Avg Reward: 380.30\n",
      "Global Step: 41984, steps/s: 204, Avg Reward: 381.80\n",
      "Global Step: 43008, steps/s: 204, Avg Reward: 384.10\n",
      "Global Step: 44032, steps/s: 204, Avg Reward: 377.60\n",
      "Global Step: 45056, steps/s: 204, Avg Reward: 365.70\n",
      "Global Step: 46080, steps/s: 205, Avg Reward: 362.50\n",
      "Global Step: 47104, steps/s: 205, Avg Reward: 363.70\n",
      "Global Step: 48128, steps/s: 205, Avg Reward: 360.20\n",
      "Global Step: 49152, steps/s: 205, Avg Reward: 385.70\n",
      "Global Step: 50176, steps/s: 205, Avg Reward: 399.20\n",
      "Global Step: 51200, steps/s: 205, Avg Reward: 401.10\n",
      "Global Step: 52224, steps/s: 206, Avg Reward: 407.70\n",
      "Global Step: 53248, steps/s: 206, Avg Reward: 402.40\n",
      "Global Step: 54272, steps/s: 204, Avg Reward: 402.70\n",
      "Global Step: 55296, steps/s: 134, Avg Reward: 411.50\n",
      "Global Step: 56320, steps/s: 134, Avg Reward: 418.20\n",
      "Global Step: 57344, steps/s: 42, Avg Reward: 416.60\n",
      "Global Step: 58368, steps/s: 42, Avg Reward: 408.10\n",
      "Global Step: 59392, steps/s: 39, Avg Reward: 411.10\n",
      "Global Step: 60416, steps/s: 39, Avg Reward: 412.20\n",
      "Global Step: 61440, steps/s: 39, Avg Reward: 418.80\n",
      "Global Step: 62464, steps/s: 39, Avg Reward: 417.00\n",
      "Global Step: 63488, steps/s: 39, Avg Reward: 417.90\n",
      "Global Step: 64512, steps/s: 40, Avg Reward: 382.70\n",
      "Global Step: 65536, steps/s: 40, Avg Reward: 368.90\n",
      "Global Step: 66560, steps/s: 41, Avg Reward: 365.30\n",
      "Global Step: 67584, steps/s: 41, Avg Reward: 358.20\n",
      "Global Step: 68608, steps/s: 42, Avg Reward: 366.00\n",
      "Global Step: 69632, steps/s: 42, Avg Reward: 354.80\n",
      "Global Step: 70656, steps/s: 43, Avg Reward: 361.50\n",
      "Global Step: 71680, steps/s: 43, Avg Reward: 362.40\n",
      "Global Step: 72704, steps/s: 44, Avg Reward: 362.40\n",
      "Global Step: 73728, steps/s: 44, Avg Reward: 384.20\n",
      "Global Step: 74752, steps/s: 45, Avg Reward: 383.40\n",
      "Global Step: 75776, steps/s: 45, Avg Reward: 374.90\n",
      "Global Step: 76800, steps/s: 46, Avg Reward: 378.10\n",
      "Global Step: 77824, steps/s: 46, Avg Reward: 389.10\n",
      "Global Step: 78848, steps/s: 47, Avg Reward: 373.60\n",
      "Global Step: 79872, steps/s: 47, Avg Reward: 373.10\n",
      "Global Step: 80896, steps/s: 48, Avg Reward: 364.60\n",
      "Global Step: 81920, steps/s: 48, Avg Reward: 365.90\n",
      "Global Step: 82944, steps/s: 49, Avg Reward: 368.10\n",
      "Global Step: 83968, steps/s: 49, Avg Reward: 366.10\n",
      "Global Step: 84992, steps/s: 50, Avg Reward: 366.90\n",
      "Global Step: 86016, steps/s: 50, Avg Reward: 358.90\n",
      "Global Step: 87040, steps/s: 51, Avg Reward: 350.40\n",
      "Global Step: 88064, steps/s: 51, Avg Reward: 333.60\n",
      "Global Step: 89088, steps/s: 51, Avg Reward: 327.40\n",
      "Global Step: 90112, steps/s: 52, Avg Reward: 328.30\n",
      "Global Step: 91136, steps/s: 52, Avg Reward: 357.10\n",
      "Global Step: 92160, steps/s: 53, Avg Reward: 356.70\n",
      "Global Step: 93184, steps/s: 53, Avg Reward: 357.60\n",
      "Global Step: 94208, steps/s: 54, Avg Reward: 355.70\n",
      "Global Step: 95232, steps/s: 54, Avg Reward: 358.60\n",
      "Global Step: 96256, steps/s: 55, Avg Reward: 357.40\n",
      "Global Step: 97280, steps/s: 55, Avg Reward: 353.70\n",
      "Global Step: 98304, steps/s: 55, Avg Reward: 348.90\n",
      "Global Step: 99328, steps/s: 56, Avg Reward: 357.50\n",
      "Global Step: 100352, steps/s: 56, Avg Reward: 356.30\n",
      "Global Step: 101376, steps/s: 57, Avg Reward: 354.40\n",
      "Global Step: 102400, steps/s: 57, Avg Reward: 354.20\n",
      "Global Step: 103424, steps/s: 58, Avg Reward: 353.00\n",
      "Global Step: 104448, steps/s: 58, Avg Reward: 349.20\n",
      "Global Step: 105472, steps/s: 58, Avg Reward: 323.40\n",
      "Global Step: 106496, steps/s: 59, Avg Reward: 314.20\n",
      "Global Step: 107520, steps/s: 59, Avg Reward: 310.10\n",
      "Global Step: 108544, steps/s: 60, Avg Reward: 321.10\n",
      "Global Step: 109568, steps/s: 60, Avg Reward: 333.20\n",
      "Global Step: 110592, steps/s: 60, Avg Reward: 335.80\n",
      "Global Step: 111616, steps/s: 61, Avg Reward: 347.30\n",
      "Global Step: 112640, steps/s: 61, Avg Reward: 345.60\n",
      "Global Step: 113664, steps/s: 62, Avg Reward: 354.00\n",
      "Global Step: 114688, steps/s: 62, Avg Reward: 350.40\n",
      "Global Step: 115712, steps/s: 62, Avg Reward: 359.40\n",
      "Global Step: 116736, steps/s: 63, Avg Reward: 367.80\n",
      "Global Step: 117760, steps/s: 63, Avg Reward: 369.60\n",
      "Global Step: 118784, steps/s: 64, Avg Reward: 377.40\n",
      "Global Step: 119808, steps/s: 64, Avg Reward: 381.70\n",
      "Global Step: 120832, steps/s: 64, Avg Reward: 388.50\n",
      "Global Step: 121856, steps/s: 65, Avg Reward: 403.60\n",
      "Global Step: 122880, steps/s: 65, Avg Reward: 415.90\n",
      "Global Step: 123904, steps/s: 65, Avg Reward: 406.60\n",
      "Global Step: 124928, steps/s: 66, Avg Reward: 403.70\n",
      "Global Step: 125952, steps/s: 66, Avg Reward: 390.90\n",
      "Global Step: 126976, steps/s: 66, Avg Reward: 389.30\n",
      "Global Step: 128000, steps/s: 67, Avg Reward: 399.00\n",
      "Global Step: 129024, steps/s: 67, Avg Reward: 381.20\n",
      "Global Step: 130048, steps/s: 67, Avg Reward: 383.70\n",
      "Global Step: 131072, steps/s: 68, Avg Reward: 376.20\n",
      "Global Step: 132096, steps/s: 68, Avg Reward: 373.70\n",
      "Global Step: 133120, steps/s: 68, Avg Reward: 370.60\n",
      "Global Step: 134144, steps/s: 69, Avg Reward: 362.70\n",
      "Global Step: 135168, steps/s: 69, Avg Reward: 362.10\n",
      "Global Step: 136192, steps/s: 70, Avg Reward: 371.50\n",
      "Global Step: 137216, steps/s: 70, Avg Reward: 345.70\n",
      "Global Step: 138240, steps/s: 70, Avg Reward: 343.50\n",
      "Global Step: 139264, steps/s: 70, Avg Reward: 352.10\n",
      "Global Step: 140288, steps/s: 71, Avg Reward: 354.00\n",
      "Global Step: 141312, steps/s: 71, Avg Reward: 359.30\n",
      "Global Step: 142336, steps/s: 71, Avg Reward: 362.50\n",
      "Global Step: 143360, steps/s: 72, Avg Reward: 368.80\n",
      "Global Step: 144384, steps/s: 72, Avg Reward: 369.90\n",
      "Global Step: 145408, steps/s: 72, Avg Reward: 370.80\n",
      "Global Step: 146432, steps/s: 73, Avg Reward: 367.40\n",
      "Global Step: 147456, steps/s: 73, Avg Reward: 367.40\n",
      "Global Step: 148480, steps/s: 73, Avg Reward: 369.20\n",
      "Global Step: 149504, steps/s: 74, Avg Reward: 377.20\n",
      "Global Step: 150528, steps/s: 74, Avg Reward: 379.10\n",
      "Global Step: 151552, steps/s: 74, Avg Reward: 363.50\n",
      "Global Step: 152576, steps/s: 75, Avg Reward: 363.60\n",
      "Global Step: 153600, steps/s: 75, Avg Reward: 364.60\n",
      "Global Step: 154624, steps/s: 75, Avg Reward: 360.70\n",
      "Global Step: 155648, steps/s: 76, Avg Reward: 352.00\n",
      "Global Step: 156672, steps/s: 76, Avg Reward: 341.40\n",
      "Global Step: 157696, steps/s: 76, Avg Reward: 345.80\n",
      "Global Step: 158720, steps/s: 77, Avg Reward: 338.50\n",
      "Global Step: 159744, steps/s: 77, Avg Reward: 335.70\n",
      "Global Step: 160768, steps/s: 77, Avg Reward: 357.50\n",
      "Global Step: 161792, steps/s: 77, Avg Reward: 360.90\n",
      "Global Step: 162816, steps/s: 78, Avg Reward: 357.90\n",
      "Global Step: 163840, steps/s: 78, Avg Reward: 357.30\n",
      "Global Step: 164864, steps/s: 78, Avg Reward: 351.90\n",
      "Global Step: 165888, steps/s: 79, Avg Reward: 351.30\n",
      "Global Step: 166912, steps/s: 79, Avg Reward: 346.50\n",
      "Global Step: 167936, steps/s: 79, Avg Reward: 342.40\n",
      "Global Step: 168960, steps/s: 80, Avg Reward: 345.20\n",
      "Global Step: 169984, steps/s: 80, Avg Reward: 354.80\n",
      "Global Step: 171008, steps/s: 80, Avg Reward: 371.80\n",
      "Global Step: 172032, steps/s: 80, Avg Reward: 379.30\n",
      "Global Step: 173056, steps/s: 81, Avg Reward: 374.50\n",
      "Global Step: 174080, steps/s: 81, Avg Reward: 374.30\n",
      "Global Step: 175104, steps/s: 81, Avg Reward: 375.40\n",
      "Global Step: 176128, steps/s: 82, Avg Reward: 355.50\n",
      "Global Step: 177152, steps/s: 82, Avg Reward: 354.10\n",
      "Global Step: 178176, steps/s: 82, Avg Reward: 366.10\n",
      "Global Step: 179200, steps/s: 82, Avg Reward: 357.50\n",
      "Global Step: 180224, steps/s: 83, Avg Reward: 360.70\n",
      "Global Step: 181248, steps/s: 83, Avg Reward: 362.10\n",
      "Global Step: 182272, steps/s: 83, Avg Reward: 371.90\n",
      "Global Step: 183296, steps/s: 84, Avg Reward: 382.60\n",
      "Global Step: 184320, steps/s: 84, Avg Reward: 377.90\n",
      "Global Step: 185344, steps/s: 84, Avg Reward: 371.80\n",
      "Global Step: 186368, steps/s: 84, Avg Reward: 371.30\n",
      "Global Step: 187392, steps/s: 85, Avg Reward: 368.30\n",
      "Global Step: 188416, steps/s: 85, Avg Reward: 368.70\n",
      "Global Step: 189440, steps/s: 85, Avg Reward: 359.30\n",
      "Global Step: 190464, steps/s: 86, Avg Reward: 363.30\n",
      "Global Step: 191488, steps/s: 86, Avg Reward: 362.40\n",
      "Global Step: 192512, steps/s: 86, Avg Reward: 360.60\n",
      "Global Step: 193536, steps/s: 86, Avg Reward: 351.50\n",
      "Global Step: 194560, steps/s: 87, Avg Reward: 354.90\n",
      "Global Step: 195584, steps/s: 87, Avg Reward: 359.80\n",
      "Global Step: 196608, steps/s: 87, Avg Reward: 352.50\n",
      "Global Step: 197632, steps/s: 87, Avg Reward: 339.00\n",
      "Global Step: 198656, steps/s: 88, Avg Reward: 335.20\n",
      "Global Step: 199680, steps/s: 88, Avg Reward: 344.90\n",
      "Global Step: 200704, steps/s: 88, Avg Reward: 343.90\n",
      "Global Step: 201728, steps/s: 88, Avg Reward: 349.00\n",
      "Global Step: 202752, steps/s: 89, Avg Reward: 347.90\n",
      "Global Step: 203776, steps/s: 89, Avg Reward: 351.30\n",
      "Global Step: 204800, steps/s: 89, Avg Reward: 344.10\n",
      "Global Step: 205824, steps/s: 89, Avg Reward: 345.60\n",
      "Global Step: 206848, steps/s: 90, Avg Reward: 343.60\n",
      "Global Step: 207872, steps/s: 90, Avg Reward: 354.50\n",
      "Global Step: 208896, steps/s: 90, Avg Reward: 360.10\n",
      "Global Step: 209920, steps/s: 90, Avg Reward: 360.00\n",
      "Global Step: 210944, steps/s: 91, Avg Reward: 357.20\n",
      "Global Step: 211968, steps/s: 91, Avg Reward: 352.20\n",
      "Global Step: 212992, steps/s: 91, Avg Reward: 352.20\n",
      "Global Step: 214016, steps/s: 91, Avg Reward: 355.10\n",
      "Global Step: 215040, steps/s: 92, Avg Reward: 351.50\n",
      "Global Step: 216064, steps/s: 92, Avg Reward: 341.30\n",
      "Global Step: 217088, steps/s: 92, Avg Reward: 337.50\n",
      "Global Step: 218112, steps/s: 92, Avg Reward: 341.50\n",
      "Global Step: 219136, steps/s: 93, Avg Reward: 337.60\n",
      "Global Step: 220160, steps/s: 93, Avg Reward: 334.90\n",
      "Global Step: 221184, steps/s: 93, Avg Reward: 339.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/nir.k/uni/PPO-Implementation/code/ppo_train.py\", line 313, in <module>\n",
      "    final_reward, reward_plot = ppo.train()\n",
      "                                ^^^^^^^^^^^\n",
      "  File \"/Users/nir.k/uni/PPO-Implementation/code/ppo_train.py\", line 189, in train\n",
      "    self.learn()\n",
      "  File \"/Users/nir.k/uni/PPO-Implementation/code/ppo_train.py\", line 79, in learn\n",
      "    _, new_log_probs, entropy, new_values, new_dist = self.agent.get_action_and_value(b_states, b_actions)\n",
      "                                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nir.k/uni/PPO-Implementation/code/agent.py\", line 99, in get_action_and_value\n",
      "    value = self.critic(state).squeeze()\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nir.k/uni/PPO-Implementation/code/agent.py\", line 80, in forward\n",
      "    state = self.state_encoder(state)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nir.k/uni/PPO-Implementation/code/state_encoder.py\", line 66, in forward\n",
      "    return self.cnn(state)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/container.py\", line 240, in forward\n",
      "    input = module(input)\n",
      "            ^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/nir.k/uni/PPO-Implementation/code/state_encoder.py\", line 38, in forward\n",
      "    out_unf = x_unf.transpose(1, 2).matmul(w_flat.t()).transpose(1, 2)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "run_command = \"\"\"\n",
    "python ppo_train.py \\\n",
    "    --env-name \"ALE/Alien-v5\" \\\n",
    "    --state-encoder \"AtariImageEncoder\" \\\n",
    "    --discrete-action \\\n",
    "    --total-timesteps 10000000 \\\n",
    "    --num-envs 8 \\\n",
    "    --num-steps 128 \\\n",
    "    --update-epochs 3 \\\n",
    "    --minibatch-size 256 \\\n",
    "    --adam-stepsize 2.5e-4 \\\n",
    "    --gamma 0.99 \\\n",
    "    --gae-lambda 0.95 \\\n",
    "    --clip-coef 0.1 \\\n",
    "    --vf-coef 1.0 \\\n",
    "    --ent-coef 0.01 \\\n",
    "    --anneal-lr \\\n",
    "    --anneal-clip \\\n",
    "    --atari \\\n",
    "\"\"\"\n",
    "\n",
    "os.system(run_command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
